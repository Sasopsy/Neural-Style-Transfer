{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(tensor,index=0):\n",
    "    \"\"\"\n",
    "    Displays the image.\\n\\n\n",
    "\n",
    "    Arguments:\\n\n",
    "    tensor -- A 4D tensor representing an image or an array of images.\\n\n",
    "    index -- Index of image in the tensor to display. By default it will display the zeroth image.\\n\n",
    "    \"\"\"\n",
    "    plt.imshow(tensor[index])\n",
    "    plt.show()\n",
    "\n",
    "def convert_to_tensor(path,shape=(512,512,3)):\n",
    "    \"\"\"\n",
    "    Converts image into a tf.Tensor object.\\n\\n\n",
    "\n",
    "    Arguments:\\n\n",
    "    path -- Path to the image in the directory.\\n\n",
    "    shape -- Dimensions of the image (length and width).\\n\n",
    "\n",
    "    Returns:\\n\n",
    "    tensor -- Tensor of shape (1,shape,3) with float 32 values.\\n\n",
    "    \"\"\"\n",
    "    image = np.array(PIL.Image.open(path).resize(shape))\n",
    "    image = tf.constant(np.reshape(image, ((1,)+image.shape)))\n",
    "    return image\n",
    "\n",
    "def preprocess_image(tensor,dtype=tf.float32):\n",
    "    \"\"\" \n",
    "    Converts tf.Tensor tensor into given dtype and then into tf.Variable object.\\n\\n\n",
    "\n",
    "    Arguments:\\n\n",
    "    tensor -- tf.Tensor representation of the image.\\n\n",
    "    dtype -- dtype object the tensor has to be converted to.\\n\\n\n",
    "\n",
    "    Returns:\\n\n",
    "    preprocessed_image -- tf.Variable tensor of the image.\\n\n",
    "    \"\"\"\n",
    "    preprocessed_image = tf.Variable(tf.image.convert_image_dtype(tensor,dtype=dtype))\n",
    "    return preprocessed_image\n",
    "\n",
    "def random_noise_generator(image=None,mean=0.0,stddev=1.0,intensity=0.25,shape=(1,512,512,3)):\n",
    "    \"\"\"\n",
    "    Adds noise to images (tf.Tensor) or generates image with noise from the normal distribution. \\n\\n\n",
    "\n",
    "    Arguments: \\n\n",
    "    image -- tf.Tensor object representing the image.\\n\n",
    "    stddev -- Standard deviation of the distribution.\\n\n",
    "    mean -- Mean of the distribution.\\n\n",
    "    instensity -- A weight multiplied to the noise.\\n\\n\n",
    "    \n",
    "    Returns:\\n\n",
    "    noise -- A tf.Tensor representing the image with random noise.\\n\n",
    "    OR\\n\n",
    "    noisy_image -- A tf.Tensor representing the image with random noise added to it.\\n\n",
    "    \"\"\"\n",
    "    if image is not None:\n",
    "        tensor = tf.image.convert_image_dtype(image,tf.float32)\n",
    "        noise = tf.random.normal(tensor.shape,mean=mean,stddev=stddev)\n",
    "        noisy_image = tf.add(tensor,tf.multiply(intensity,noise))\n",
    "        noisy_image = tf.clip_by_value(noisy_image, clip_value_min=0.0, clip_value_max=1.0)\n",
    "    else:\n",
    "        noisy_image = tf.random.normal(shape,mean=mean,stddev=stddev)\n",
    "        noisy_image = tf.clip_by_value(noisy_image, clip_value_min=0.0, clip_value_max=1.0)\n",
    "    return noisy_image\n",
    "\n",
    "def convert_to_image(tensor):\n",
    "    \"\"\"\n",
    "    Converts the given tensor into a PIL image.\\n\\n\n",
    "    \n",
    "    Arguments:\\n\n",
    "    tensor -- Tensor.\\n\\n\n",
    "    \n",
    "    Returns:\\n\n",
    "    Image: A PIL image\\n\n",
    "    \"\"\"\n",
    "    tensor = tensor * 255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor) > 3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)\n",
    "\n",
    "def show_images_in_a_row(content_image,style_image,generated_image):\n",
    "    \"\"\"\n",
    "    Generates three subplots to compare the content, style and generated images.\\n\\n\n",
    "\n",
    "    Arguments:\\n\n",
    "    content_image: A tensor representing the content image.\\n\n",
    "    style_image: A tensor representing the style image.\\n\n",
    "    generated_image: A tensor representing the generated image.\\n\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 4))\n",
    "    ax = fig.add_subplot(1, 3, 1)\n",
    "    plt.imshow(content_image[0])\n",
    "    ax.title.set_text('Content image')\n",
    "    ax = fig.add_subplot(1, 3, 2)\n",
    "    plt.imshow(style_image[0])\n",
    "    ax.title.set_text('Style image')\n",
    "    ax = fig.add_subplot(1, 3, 3)\n",
    "    plt.imshow(generated_image[0])\n",
    "    ax.title.set_text('Generated image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Loss\n",
    "\n",
    "$$L_{content} = \\frac{1}{2}(G^{l}-A^{l})^{2}$$\n",
    "\n",
    "where,\n",
    "\n",
    "$n_{H}$ : Height of feature map of $l^{th}$ layer.\n",
    "\n",
    "$n_{W}$ : Width of feature map of $l^{th}$ layer.\n",
    "\n",
    "$n_{C}$ : Number of channels in feature map of $l^{th}$ layer.\n",
    "\n",
    "$G^{l}$ : Feature map of $l^{th}$ layer of generated image.\n",
    "\n",
    "$A^{l}$ : Feature map of $l^{th}$ layer of content image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_content_loss(a_O,a_G):\n",
    "    \"\"\"\n",
    "    Computes the content loss for a content image and a generated image.\\n\\n\n",
    "\n",
    "    Arguments:\\n\n",
    "    a_O -- List of the content tf.Tensor's of shape (1,n_H,n_W,n_C).\\n\n",
    "    a_G -- List of the generated tf.Tensor's of shape (1,n_H,n_W,n_C).\\n\\n\n",
    "\n",
    "    Returns:\\n\n",
    "    L_content -- Total content loss.\n",
    "    \"\"\"\n",
    "\n",
    "    #Extract last array from the a_0, a_G since it's the feature map of the content layer.\n",
    "    a_O = a_O[-1]\n",
    "    a_G = a_G[-1]\n",
    "\n",
    "    #Extract dimensions\n",
    "    _, n_H, n_W, n_C = a_O.shape\n",
    "\n",
    "    #Compute cost.\n",
    "    cost = 0.5*tf.square(tf.subtract(a_G,a_O))\n",
    "\n",
    "    #Compute total loss\n",
    "    L_content = tf.reduce_sum(cost)\n",
    "\n",
    "    return L_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gram Matrix\n",
    "\n",
    "$$Gram = A^{T}A$$\n",
    "\n",
    "where,\n",
    "\n",
    "$A$ : Unrolled tensor matrix with shape $(n_{H}\\times n_{W},n_{C})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    \"\"\" \n",
    "    Shrinks dimension of tensor to 3D and computes the gram matrix.\\n\\n\n",
    "\n",
    "    Arguments:\\n\n",
    "    tensor -- tf.Tensor of shape (1,n_H,n_W,n_C).\\n\\n\n",
    "\n",
    "    Returns:\n",
    "    gram -- Gram matrix of shape (n_H*n_W,n_C).\\n\n",
    "    \"\"\"\n",
    "    #Extracting 3D array from a 4D one.\n",
    "    tensor = tensor[-1]\n",
    "\n",
    "    #Get shape of the feature map.\n",
    "    n_H, n_W, n_C = tensor.shape\n",
    "\n",
    "    #Unrolling the tensor into 2D matrix.\n",
    "    tensor_unrolled = tf.reshape(tensor,shape=[n_H*n_W,n_C])\n",
    "\n",
    "    #Computing gram matrix.\n",
    "    gram = tf.matmul(tf.transpose(tensor_unrolled),tensor_unrolled)\n",
    "    \n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Loss\n",
    "\n",
    "$$L_{l} = \\frac{1}{4 \\times (n_{H} \\times n_{W} \\times n_{C})^{2}}(G_{gram}^{l}-S_{gram}^{l})^{2}$$\n",
    "\n",
    "where,\n",
    "\n",
    "$n_{H}$ : Height of feature map of $l^{th}$ layer.\n",
    "\n",
    "$n_{W}$ : Width of feature map of $l^{th}$ layer.\n",
    "\n",
    "$n_{C}$ : Number of channels in feature map of $l^{th}$ layer.\n",
    "\n",
    "$G_{gram}^{l}$ : Gram matrix of the feature map of $l^{th}$ layer of generated image.\n",
    "\n",
    "$S_{gram}^{l}$ : Gram matrix of the feature map of $l^{th}$ layer of style image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer_style_loss(a_S,a_G):\n",
    "    \"\"\" \n",
    "    Computes style loss of feature maps produced by a certain layer.\n",
    "\n",
    "    Arguments:\\n\n",
    "    a_S -- The style tf.Tensor of shape (1,n_H,n_W,n_C).\\n\n",
    "    a_G -- The generated tf.Tensor of shape (1,n_H,n_W,n_C).\\n\\n\n",
    "\n",
    "    Returns:\n",
    "    layer_loss -- Stlye loss of the layer.\n",
    "    \"\"\"\n",
    "\n",
    "    #Extracting dimensions of tensors\n",
    "    n_H, n_W, n_C = a_S[-1].shape\n",
    "    \n",
    "    #Gram matrix of generated image feature map.\n",
    "    generated_gram = gram_matrix(a_G)\n",
    "\n",
    "    #Gram matrix of style image feature map.\n",
    "    style_gram = gram_matrix(a_S)\n",
    "\n",
    "    #Layer cost\n",
    "    layer_cost = (1.0/(4.0*(n_H*n_W*n_C)*(n_H*n_W*n_C)))*tf.square(tf.subtract(generated_gram,style_gram))\n",
    "\n",
    "    #Layer loss\n",
    "    layer_loss = tf.reduce_sum(layer_cost)\n",
    "    \n",
    "    return layer_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Style Loss\n",
    "\n",
    "$$L_{style} = \\sum_{l=1}^n w_{l}L_{l}$$\n",
    "\n",
    "where,\n",
    "\n",
    "$w_{l}$ : Weight of the loss of the $l_{th}$ layer.\n",
    "\n",
    "$L_{l}$ : Loss of the $l_{th}$ layer.\n",
    "\n",
    "$n$ : Number of style layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_style_loss(layer_outputs_generated,layer_outputs_style,w):\n",
    "    \"\"\" \n",
    "    Computes total loss of all the layers.\\n\\n\n",
    "\n",
    "    Arguments:\\n\n",
    "    layer_outputs_generated -- List of feature maps of the generated image.\\n\n",
    "    layer_outputs_style -- List of feature maps of the style image.\\n\n",
    "    w -- Weight added to each layer loss.\\n\\n\n",
    "\n",
    "    Returns:\n",
    "    L_style -- Total style loss of all the layers.\\n\n",
    "    \"\"\"\n",
    "    \n",
    "    #Initiating variable for total loss of all the layers.\n",
    "    L_style = 0.0\n",
    "\n",
    "    #The last layer output in the string is the content representation.\n",
    "    a_S = layer_outputs_style[:-1]\n",
    "    a_G = layer_outputs_generated[:-1]\n",
    "\n",
    "    #Iterating through the loop of a list of layer outputs of generated and style image feature, multiplying them by w and adding them up.\n",
    "    for i in range(len(a_G)):\n",
    "        L_layer = compute_layer_style_loss(a_S[i],a_G[i])\n",
    "        L_style += w*L_layer\n",
    "\n",
    "    return L_style\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Loss\n",
    "\n",
    "$$ L = \\alpha L_{content} + \\beta L_{style}$$\n",
    "\n",
    "where,\n",
    "\n",
    "$\\alpha$ : Weight on content loss.\n",
    "\n",
    "$\\beta$ : Weight on style loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_total_loss(L_content,L_style,alpha=1e-1,beta=1e-2):\n",
    "    \"\"\"\n",
    "    Computes the total loss.\\n\\n\n",
    "    \n",
    "    Arguments:\\n\n",
    "    L_content -- Content loss.\\n\n",
    "    L_style -- Style loss.\\n\n",
    "    alpha -- Weight of content loss.\\n\n",
    "    beta -- Weight of style loss.\\n\\n\n",
    "    \n",
    "    Returns:\\n\n",
    "    L -- Total loss.\\n\n",
    "    \"\"\"\n",
    "\n",
    "    L = tf.add(alpha*L_content,beta*L_style)\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Style and Content layers whose feature maps we want to get.\n",
    "\n",
    "STYLE_LAYERS = (\n",
    "    'block1_conv1',\n",
    "    'block2_conv1',\n",
    "    'block3_conv1',\n",
    "    'block4_conv1',\n",
    "    'block5_conv1')\n",
    "\n",
    "CONTENT_LAYER = (\n",
    "    'block5_conv2',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_outputs(model,STYLE_LAYERS,CONTENT_LAYER):\n",
    "    \"\"\" \n",
    "    Returns a model to compute feature maps at the desired layers.\\n\n",
    "    This model returns a list of feature maps (Tensor object) from the desired layers in the tuple arguments. The last tensor is the content map.\\n\\n\n",
    "\n",
    "    Arguments:\\n\n",
    "    model -- A tf.keras.Model object whose layer outputs we want.\\n\n",
    "    STYLE_LAYER -- A tuple with the names of style layers.\\n\n",
    "    CONTENT_LAYER -- A tuple with the names of content layers.\\n\\n\n",
    "\n",
    "    Returns:\\n\n",
    "    output_model -- Model to compute feature maps from the desired layers.\\n\n",
    "    \"\"\" \n",
    "    #Define a list which will contain the outputs of the layers from the tuples in the argument.\n",
    "    outputs = []\n",
    "\n",
    "    #Iterate through the STYLE_LAYERS and append them to the list.\n",
    "    for layer in STYLE_LAYERS:\n",
    "        outputs.append(model.get_layer(layer).output)\n",
    "    \n",
    "    #Iterate through the CONTENT_LAYERS and append them to the list.\n",
    "    for layer in CONTENT_LAYER:\n",
    "        outputs.append(model.get_layer(layer).output)\n",
    "\n",
    "    #Define the model with its inputs and outputs\n",
    "    output_model = tf.keras.Model(inputs = [model.input], outputs = outputs)\n",
    "\n",
    "    return output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model,generated_image,encoded_style,encoded_content,optimizer,alpha=1e-1,beta=1e-2):\n",
    "    \"\"\"\n",
    "    Returns the optimized generated image after a single iteration and the loss as well.\\n\\n\n",
    "    \n",
    "    Arguments:\\n\n",
    "    model -- tf.keras.Model from which we want to get the feature map.\\n\n",
    "    generated_image -- Tensor of the generated image.\\n\n",
    "    encoded_style -- Feature maps of the style image.\\n\n",
    "    encoded_content -- Feature maps of the content image.\\n\n",
    "    optimizer -- Optimizer.\\n \n",
    "    alpha -- Weight on content loss.\\n\n",
    "    beta -- weight on style loss.\\n\\n\n",
    "    \n",
    "    Returns:\\n\n",
    "    generated_image -- Generated image tensor optimized after a single train step.\\n\n",
    "    L -- Total loss of train step.\\n\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        tape.watch(generated_image)\n",
    "\n",
    "        #Compute feature map generated image.\n",
    "        a_G = model(generated_image)\n",
    "\n",
    "        #Compute style loss.\n",
    "        L_style = compute_style_loss(a_G,encoded_style,w=0.2)\n",
    "\n",
    "        #Compute content loss.\n",
    "        L_content = compute_content_loss(encoded_content,a_G)\n",
    "\n",
    "        #Compute total loss.\n",
    "        L = compute_total_loss(L_content,L_style,alpha,beta)\n",
    "    \n",
    "    #Calculate gradients.\n",
    "    grad = tape.gradient(L,generated_image)\n",
    "\n",
    "    #Optimise image.\n",
    "    optimizer.apply_gradients([(grad,generated_image)])\n",
    "\n",
    "    #Clip the values over 1 and lesser than 0 in the tensor.\n",
    "    generated_image = tf.clip_by_value(generated_image,0,1)\n",
    "\n",
    "    return generated_image,L\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = convert_to_tensor(path='images_organised/BITS_Van_Gogh/BITS_Content.jpeg',shape = (512,512))\n",
    "style_image = convert_to_tensor(path='images_organised/BITS_Van_Gogh/Van_Gogh_Style_Image.jpeg',shape =(512,512))\n",
    "generated_image = random_noise_generator(image=content_image,intensity=0.0)\n",
    "\n",
    "show_images_in_a_row(content_image,style_image,generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the images.\n",
    "content_image = preprocess_image(content_image)\n",
    "style_image = preprocess_image(style_image)\n",
    "generated_image = preprocess_image(generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vgg19 = tf.keras.applications.vgg19.VGG19(include_top=False,input_shape=[512,512,3])\n",
    "#tf.keras.models.save_model(model=vgg19,filepath='vgg19_model.keras')\n",
    "\n",
    "#Loading the VGG-19 model from the directory.\n",
    "vgg19 = tf.keras.models.load_model('vgg19_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model.\n",
    "vgg_model_outputs = model_outputs(vgg19,STYLE_LAYERS=STYLE_LAYERS,CONTENT_LAYER=CONTENT_LAYER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Style and content representations feature maps.\n",
    "content_encoded = vgg_model_outputs(content_image)\n",
    "style_encoded = vgg_model_outputs(style_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.01,beta_1=0.99)\n",
    "print_every = 10\n",
    "path = 'images_organised/BITS_Van_Gogh/Generated_Images/BITS_Van_Gogh_3'\n",
    "save_every = 200\n",
    "\n",
    "for i in range(epochs+1):\n",
    "    \n",
    "    #Get generated image and loss of a single train step.\n",
    "    generated_image,l = train_step(model = vgg_model_outputs,\n",
    "                                 generated_image = generated_image, \n",
    "                                 encoded_style = style_encoded,\n",
    "                                 encoded_content = content_encoded,\n",
    "                                 optimizer = optimizer)\n",
    "    \n",
    "    #Reconvert generated image to tf.Variable object.\n",
    "    generated_image = tf.Variable(generated_image)\n",
    "    \n",
    "    #Print every print_every epoch.\n",
    "    if i%print_every == 0:\n",
    "        print(f\"Epoch: {i}\")\n",
    "        print(f\"Loss: {l}\")\n",
    "        image = convert_to_image(generated_image)\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "    \n",
    "    #Save every save_every epoch.\n",
    "    if i%save_every == 0:\n",
    "        image = convert_to_image(generated_image)\n",
    "        image.save(path+f'_{i}.jpeg')\n",
    "        \n",
    "\n",
    "image = convert_to_image(generated_image)\n",
    "image.save(path+'.jpeg')\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_in_a_row(content_image=content_image,style_image=style_image,generated_image=generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
